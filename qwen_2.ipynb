{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc4d8431-ca20-40c5-91bb-0c3547b31ba7",
   "metadata": {},
   "source": [
    "# 공통함수 및 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e284fe0-dc26-4215-aa75-b38c76c1eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f88e5ae8e87478c8b82bd2f034f8381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999d986f548b48de9971a435f9dcbe24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.lerobot.datasets.lerobot_dataset import LeRobotDataset, LeRobotDatasetMetadata\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n",
    "from lerobot.policies.act.modeling_act import ACTPolicy\n",
    "\n",
    "dataset = LeRobotDataset(\n",
    "    repo_id=\"lerobot/custom_dataset\",\n",
    "    root='/mnt/d/lerobot_dataset/panda_robot_dataset_lerobot/v1_latent',\n",
    "    episodes = list(range(700, 1000))\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    #num_workers=8,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    #pin_memory=device.type != \"cpu\",\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "def inputs_maker(processor, batch, i):\n",
    "    ImageSet = []\n",
    "\n",
    "    tasks = batch['task']\n",
    "    \n",
    "    pil_img_m = np.transpose((batch['observation.mid_image'][0].numpy() * 255), [1,2,0])\n",
    "    pil_img_r = np.transpose((batch['observation.right_image'][0].numpy() * 255), [1,2,0])\n",
    "    pil_img_l = np.transpose((batch['observation.left_image'][0].numpy() * 255), [1,2,0])\n",
    "\n",
    "    ImageSet.append(pil_img_m)\n",
    "    ImageSet.append(pil_img_r)\n",
    "    ImageSet.append(pil_img_l)\n",
    "    \n",
    "    texts = [(\n",
    "        '<|im_start|>system\\n'\n",
    "        'You are a single-arm gripper-type robot. You have just received the following images from three cameras. '\n",
    "        'Picture 1: <|vision_start|><|image_pad|><|vision_end|> - Captured by the wrist-mounted camera. '\n",
    "        'Picture 2: <|vision_start|><|image_pad|><|vision_end|> - Captured by the rear-right camera, showing both the robot and its environment. '\n",
    "        'Picture 3: <|vision_start|><|image_pad|><|vision_end|> - Captured by the front-left camera, showing both the robot and its environment.<|im_end|>\\n'\n",
    "        '<|im_start|>user\\n'\n",
    "        f'How should you move when you need to {task}?'\n",
    "        '<|im_start|>assistant\\n'\n",
    "    )\n",
    "             for task in tasks\n",
    "            ]\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=ImageSet,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", use_fast=True)\n",
    "processor.tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "inputs = inputs_maker(processor, batch, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb6bcb-598a-40c8-86a7-a86ed52eb337",
   "metadata": {},
   "source": [
    "# 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e443eac5-9532-4c96-a1a3-5a71e56db341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/etri1234/miniconda3/envs/lerobot_env2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1e-06` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1 : tensor([[ 0.1719,  0.0010,  0.4961,  ..., -0.2148, -0.0132, -0.5273]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "output1 = model.generate(**inputs,\n",
    "                         max_new_tokens=1,\n",
    "                         do_sample=False,\n",
    "                         output_hidden_states=True,\n",
    "                         return_dict_in_generate=True)['hidden_states'][0][10][:, -1, :]\n",
    "print('output1 :', output1)\n",
    "\n",
    "#while len(model.model.layers) > 11:\n",
    "#    model.model.layers.pop(-1)\n",
    "\n",
    "#output2 = model.generate(**inputs,\n",
    "#              max_new_tokens = 1,\n",
    "#              do_sample = False,\n",
    "#              output_hidden_states = True,\n",
    "#              return_dict_in_generate = True)['hidden_states'][0][-1][:, -1, :]\n",
    "#print('output2 :', output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ad98e8-c844-4fbf-86b2-2ff35c460488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def temp_truncate(model, n_layers: int):\n",
    "    saved = model.model.layers\n",
    "    try:\n",
    "        model.model.layers = torch.nn.ModuleList(list(saved)[:n_layers])\n",
    "        yield\n",
    "    finally:\n",
    "        model.model.layers = saved\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    # 1) 풀 모델로 10번째 블록 출력 뽑기 (임베딩=0, 블록=1..N)\n",
    "    out_full = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "    h10_full = out_full.hidden_states[10][:, -1, :]\n",
    "\n",
    "    with temp_truncate(model, 10):\n",
    "        out_trunc = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "        h10_trunc = out_trunc.hidden_states[-1][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75be8532-142f-40f6-a48c-3251d9f8293c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1719,  0.0010,  0.4961,  ..., -0.2148, -0.0132, -0.5273]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h10_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da0a3975-c8ed-4261-a70b-64739b3c8851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6484,  0.0037,  1.8750,  ..., -0.8438, -0.0493, -1.9766]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h10_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac69602d-ad03-4bba-b96d-a2e90ddbf872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5ff182-c912-4485-813e-0651eadf45f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999ce215-b0db-4086-80af-253e494e4cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6484,  0.0037,  1.8750,  ..., -0.8438, -0.0493, -1.9766]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_fast.hidden_states[-1][:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac3eb01-094b-4a24-8f54-f39cdf1b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del model_fast\n",
    "del out_full\n",
    "del out_fast\n",
    "del h10_full\n",
    "del h10_fast\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b8cf8f20-95a3-482a-95a6-3bf45defcce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h10_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mh10_full\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'h10_full' is not defined"
     ]
    }
   ],
   "source": [
    "h10_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7e298e-b5c3-4f56-83d2-012a843641c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40b0036-ec55-4f8c-8d8a-933d400ac5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e6949d4-d6ef-471d-8372-4a04f3449afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[검증1] hidden_states[k+1] vs hook_outs[k]  (k=0..27)\n",
      "max |diff| per layer: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 76.5]\n",
      "allclose per layer: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False]\n",
      "[검증2] 풀모델의 hidden_states[10]  <->  trunc의 hidden_states[-1]\n",
      "max |diff|: 68.0\n",
      "allclose: False\n"
     ]
    }
   ],
   "source": [
    "import copy, torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# --- 0) 결정론/평가 모드(가능한 범위) ---\n",
    "model.eval()\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False  # 두 실행 모두 동일하게\n",
    "# (FA2는 완전 결정론 보장이 아니지만, 같은 경로/설정이면 보통 동일합니다)\n",
    "\n",
    "# --- 1) 레이어 훅 세팅: 각 Transformer block의 출력을 캡처 ---\n",
    "hook_outs = {}  # {layer_idx: tensor}\n",
    "hooks = []\n",
    "\n",
    "def make_hook(i):\n",
    "    def _hook(module, inputs, output):\n",
    "        # 블록의 forward 반환이 tensor 또는 tuple일 수 있으므로 방어적으로 처리\n",
    "        out = output[0] if isinstance(output, (tuple, list)) else output\n",
    "        hook_outs[i] = out\n",
    "    return _hook\n",
    "\n",
    "# 모델의 Transformer 블록 리스트 (Qwen: model.model.layers)\n",
    "layers = model.model.layers\n",
    "num_layers = len(layers)\n",
    "\n",
    "for i, layer in enumerate(layers):\n",
    "    hooks.append(layer.register_forward_hook(make_hook(i)))\n",
    "\n",
    "# --- 2) 입력 준비 (질문에서 쓰신 inputs_maker 그대로) ---\n",
    "batch = next(iter(dataloader))\n",
    "inputs = inputs_maker(processor, batch, 0)\n",
    "inputs = {k: (v.to(model.device) if torch.is_tensor(v) else v) for k, v in inputs.items()}\n",
    "\n",
    "# --- 3) 풀모델 1회 forward: hidden_states와 hook 결과를 1:1 검증 ---\n",
    "with torch.inference_mode():\n",
    "    out_full = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "hs = out_full.hidden_states  # 길이 = num_layers + 1 (임베딩 + 각 블록)\n",
    "assert len(hs) == num_layers + 1, (len(hs), num_layers)\n",
    "\n",
    "# 각 레이어에 대해 hidden_states[k+1] vs hook_outs[k] 동일성 확인\n",
    "max_diffs = []\n",
    "for k in range(num_layers):\n",
    "    # 시퀀스의 마지막 토큰만 비교(전체 비교도 가능)\n",
    "    a = hs[k+1][:, -1, :]\n",
    "    b = hook_outs[k][:, -1, :]\n",
    "\n",
    "    diff = (a - b).abs().max().item()\n",
    "    max_diffs.append(diff)\n",
    "\n",
    "print(f\"[검증1] hidden_states[k+1] vs hook_outs[k]  (k=0..{num_layers-1})\")\n",
    "print(\"max |diff| per layer:\", max_diffs)\n",
    "print(\"allclose per layer:\", [torch.allclose(hs[k+1][:, -1, :], hook_outs[k][:, -1, :], atol=1e-5, rtol=1e-5) for k in range(num_layers)])\n",
    "\n",
    "# --- 4) 10번째 블록 출력(=hidden_states[10])을 '정답'으로 삼음 ---\n",
    "h10_full = hs[10][:, -1, :].contiguous()\n",
    "\n",
    "# --- 5) trunc 실행: 같은 모델에서 '실행만' 10층으로 제한 ---\n",
    "@contextmanager\n",
    "def temp_truncate(m, n_layers: int):\n",
    "    saved = m.model.layers\n",
    "    try:\n",
    "        m.model.layers = torch.nn.ModuleList(list(saved)[:n_layers])\n",
    "        yield\n",
    "    finally:\n",
    "        m.model.layers = saved\n",
    "\n",
    "with torch.inference_mode():\n",
    "    with temp_truncate(model, 10):\n",
    "        out_trunc = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "# trunc의 최종 블록 출력은 hidden_states[-1] == hidden_states[10]이어야 함\n",
    "h_last_trunc = out_trunc.hidden_states[-1][:, -1, :].contiguous()\n",
    "\n",
    "# --- 6) 동일성 체크 ---\n",
    "diff_trunc = (h10_full - h_last_trunc).abs().max().item()\n",
    "print(\"[검증2] 풀모델의 hidden_states[10]  <->  trunc의 hidden_states[-1]\")\n",
    "print(\"max |diff|:\", diff_trunc)\n",
    "print(\"allclose:\", torch.allclose(h10_full, h_last_trunc, atol=1e-5, rtol=1e-5))\n",
    "\n",
    "# --- 7) 훅 해제 ---\n",
    "for h in hooks:\n",
    "    h.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d4797f-6290-4ec5-9cfe-c21815b01353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[확인A-1] hs[-1] vs final_norm(hook_outs[last])  max|diff| = 0.0\n",
      "[확인A-2] hs[-1] vs (norm 훅 출력) allclose = True\n",
      "[확인B] full norm(hs[10])  vs  trunc hs[-1]   max|diff| = 0.0\n",
      "allclose: True\n"
     ]
    }
   ],
   "source": [
    "import copy, torch\n",
    "from contextlib import contextmanager\n",
    "\n",
    "model.eval()\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "\n",
    "# 0) 최종 정규화 모듈 식별 (Qwen: 대개 model.model.norm; 일부 모델은 ln_f)\n",
    "final_norm = getattr(model.model, \"norm\", None) or getattr(model.model, \"ln_f\", None)\n",
    "assert final_norm is not None, \"final norm 모듈을 찾지 못했습니다 (model.model.norm 또는 ln_f).\"\n",
    "\n",
    "# 1) 블록 출력 훅\n",
    "hook_outs = {}\n",
    "hooks = []\n",
    "def make_hook(i):\n",
    "    def _hook(module, inputs, output):\n",
    "        out = output[0] if isinstance(output, (tuple, list)) else output\n",
    "        hook_outs[i] = out\n",
    "    return _hook\n",
    "\n",
    "layers = model.model.layers\n",
    "num_layers = len(layers)\n",
    "for i, layer in enumerate(layers):\n",
    "    hooks.append(layer.register_forward_hook(make_hook(i)))\n",
    "\n",
    "# 2) final norm 훅\n",
    "norm_out = {}\n",
    "def norm_hook(module, inputs, output):\n",
    "    norm_out[\"out\"] = output\n",
    "h_norm = final_norm.register_forward_hook(norm_hook)\n",
    "\n",
    "# 3) 입력 만들기 (당신의 inputs_maker 사용)\n",
    "batch = next(iter(dataloader))\n",
    "inputs = inputs_maker(processor, batch, 0)\n",
    "inputs = {k:(v.to(model.device) if torch.is_tensor(v) else v) for k,v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    out_full = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "hs = out_full.hidden_states  # len = num_layers + 1\n",
    "\n",
    "# --- A. 마지막 레이어 불일치 원인 확인: hs[-1] == final_norm(hook_outs[last]) ---\n",
    "last_block_raw = hook_outs[num_layers - 1]              # 마지막 블록 생 출력\n",
    "last_block_post = final_norm(last_block_raw)            # 정규화 후\n",
    "assert \"out\" in norm_out\n",
    "print(\"[확인A-1] hs[-1] vs final_norm(hook_outs[last])  max|diff| =\",\n",
    "      (hs[-1] - last_block_post).abs().max().item())\n",
    "print(\"[확인A-2] hs[-1] vs (norm 훅 출력) allclose =\",\n",
    "      torch.allclose(hs[-1], norm_out[\"out\"], atol=1e-5, rtol=1e-5))\n",
    "\n",
    "# 4) 임시 트렁케이트 유틸\n",
    "@contextmanager\n",
    "def temp_truncate(m, n_layers: int):\n",
    "    saved = m.model.layers\n",
    "    try:\n",
    "        m.model.layers = torch.nn.ModuleList(list(saved)[:n_layers])\n",
    "        yield\n",
    "    finally:\n",
    "        m.model.layers = saved\n",
    "\n",
    "# 5) “같은 기준”으로 비교: (정규화 후 기준)\n",
    "with torch.inference_mode():\n",
    "    with temp_truncate(model, 10):\n",
    "        # trunc 실행에서도 final norm 훅으로 캡처\n",
    "        norm_out_tr = {}\n",
    "        def norm_hook_tr(m, i, o): norm_out_tr[\"out\"] = o\n",
    "        h_norm_tr = final_norm.register_forward_hook(norm_hook_tr)\n",
    "\n",
    "        out_trunc = model(**inputs, use_cache=False, output_hidden_states=True, return_dict=True)\n",
    "        hs_trunc_last = out_trunc.hidden_states[-1]  # trunc의 마지막 = 정규화 후 값\n",
    "\n",
    "        h_norm_tr.remove()\n",
    "\n",
    "# 풀모델의 10번째 블록 “정규화 후” vs trunc 마지막(=정규화 후)\n",
    "h10_full_pre  = hs[10]\n",
    "h10_full_post = final_norm(h10_full_pre)\n",
    "\n",
    "print(\"[확인B] full norm(hs[10])  vs  trunc hs[-1]   max|diff| =\",\n",
    "      (h10_full_post - hs_trunc_last).abs().max().item())\n",
    "print(\"allclose:\", torch.allclose(h10_full_post, hs_trunc_last, atol=1e-5, rtol=1e-5))\n",
    "\n",
    "# 6) 훅 해제\n",
    "for h in hooks: h.remove()\n",
    "h_norm.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1dca3c-7f2b-4763-b1dc-cbe18eac8371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot_env2_kernel",
   "language": "python",
   "name": "lerobot_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
